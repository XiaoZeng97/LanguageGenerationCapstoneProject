{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fighting_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shengruilyu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../after_regex_large.csv\", index_col = 0, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_by_subreddit= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_postags = set(['NN', 'NNS', 'NNP', 'NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_by_subreddit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_by_subreddit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    sub_reddit = row['subreddit']\n",
    "    if sub_reddit in posts_by_subreddit:\n",
    "        posts_by_subreddit[sub_reddit].append(row['body'])\n",
    "    else:\n",
    "        posts_by_subreddit[sub_reddit] = [row['body']]\n",
    "    tokens = tknzr.tokenize(row['body'])\n",
    "    if sub_reddit in tokens_by_subreddit:\n",
    "        tokens_by_subreddit[sub_reddit].append(tokens)\n",
    "    else:\n",
    "        tokens_by_subreddit[sub_reddit] = [tokens]\n",
    "    \n",
    "    if sub_reddit in word_frequency_by_subreddit:\n",
    "        word_frequency = word_frequency_by_subreddit[sub_reddit]\n",
    "    else:\n",
    "        word_frequency = defaultdict(int)\n",
    "        word_frequency_by_subreddit[sub_reddit] = word_frequency\n",
    "    for token in tokens:\n",
    "        if (token not in stop_words) and (token.isalnum()):\n",
    "            word_frequency[token]+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = list(posts_by_subreddit.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['changemyview',\n",
       " 'YouShouldKnow',\n",
       " 'AskMen',\n",
       " 'Futurology',\n",
       " 'Art',\n",
       " 'IAmA',\n",
       " 'science',\n",
       " 'Cooking']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_per_by_subreddit = {}\n",
    "results_by_subreddit = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changemyview\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "YouShouldKnow\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "AskMen\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "Futurology\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "Art\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "IAmA\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "science\n",
      "Vocab size is 15000\n",
      "Comparing language...\n",
      "Cooking\n",
      "Vocab size is 15000\n",
      "Comparing language...\n"
     ]
    }
   ],
   "source": [
    "for current_subreddit in subreddits:\n",
    "    l1 = posts_by_subreddit[current_subreddit]\n",
    "    l2 = []\n",
    "    for sr in subreddits:\n",
    "        if sr == current_subreddit:\n",
    "            print (sr)\n",
    "            continue\n",
    "        l2 += posts_by_subreddit[sr]\n",
    "    results = fighting_words.bayes_compare_language(l1, l2)\n",
    "    sort_results = sorted(results, key = lambda x : x[1], reverse = True)\n",
    "    results_by_subreddit[current_subreddit] = sort_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = set([])\n",
    "all_topics = set([])\n",
    "for sr in results_by_subreddit.keys():\n",
    "    for topic in topic_per_by_subreddit[sr]:\n",
    "        if topic in all_topics:\n",
    "            duplicate.add(topic)\n",
    "        all_topics.add(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'animals',\n",
       " 'cats',\n",
       " 'energy',\n",
       " 'got',\n",
       " 'guys',\n",
       " 'like',\n",
       " 'looks',\n",
       " 'love',\n",
       " 'meat',\n",
       " 'nuclear',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'virus',\n",
       " 'women'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from duplicate\n",
    "duplicate.remove('animals')\n",
    "duplicate.remove('cats')\n",
    "duplicate.remove('energy')\n",
    "duplicate.remove('meat')\n",
    "duplicate.remove('nuclear')\n",
    "duplicate.remove('virus')\n",
    "duplicate.remove('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'got', 'guys', 'like', 'looks', 'love', 'thank', 'thanks'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicate_topic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_subreddit in subreddits:\n",
    "    topics = []\n",
    "    sort_results = results_by_subreddit[current_subreddit]\n",
    "    for word, z in sort_results:\n",
    "        if len(topics) == topk:\n",
    "            break\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if remove_duplicate_topic and word in duplicate:\n",
    "            continue\n",
    "        topics.append(word)\n",
    "    topic_per_by_subreddit[current_subreddit] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'changemyview': ['view',\n",
       "  'trump',\n",
       "  'biden',\n",
       "  'vote',\n",
       "  'comments',\n",
       "  'rule',\n",
       "  'op',\n",
       "  'argument',\n",
       "  'must',\n",
       "  'explained',\n",
       "  'within',\n",
       "  'comment',\n",
       "  'delta',\n",
       "  'breaking',\n",
       "  'page',\n",
       "  'message',\n",
       "  'posted',\n",
       "  'party',\n",
       "  'review',\n",
       "  'moderators',\n",
       "  'appeal',\n",
       "  'gender',\n",
       "  'notice',\n",
       "  'wiki',\n",
       "  'standards',\n",
       "  'ban',\n",
       "  'gun',\n",
       "  'note',\n",
       "  'believe',\n",
       "  'sexual',\n",
       "  'user',\n",
       "  'link',\n",
       "  'arguments',\n",
       "  'process',\n",
       "  'voting',\n",
       "  'change',\n",
       "  'rights',\n",
       "  'point',\n",
       "  'information',\n",
       "  'person',\n",
       "  'nt',\n",
       "  'women',\n",
       "  'system',\n",
       "  'favor',\n",
       "  'argue',\n",
       "  'lead',\n",
       "  'trans',\n",
       "  'agree',\n",
       "  'clicking',\n",
       "  'evidence',\n",
       "  'rape',\n",
       "  'multiple',\n",
       "  'moral',\n",
       "  'would',\n",
       "  'crime',\n",
       "  'conversation',\n",
       "  'bernie',\n",
       "  'election',\n",
       "  'democrats',\n",
       "  'men',\n",
       "  'rude',\n",
       "  'stated',\n",
       "  'example',\n",
       "  'court',\n",
       "  'appeals',\n",
       "  'aspect',\n",
       "  'voters',\n",
       "  'definition',\n",
       "  'consent',\n",
       "  'minor',\n",
       "  'people',\n",
       "  'police',\n",
       "  'gay',\n",
       "  'candidate',\n",
       "  'president',\n",
       "  'willing',\n",
       "  'falls',\n",
       "  'moderation',\n",
       "  'assault',\n",
       "  'victim',\n",
       "  'progressive',\n",
       "  'however',\n",
       "  'society',\n",
       "  'challenge',\n",
       "  'responses',\n",
       "  'republicans',\n",
       "  'democratic',\n",
       "  'agreement',\n",
       "  'jim',\n",
       "  'identity',\n",
       "  'race',\n",
       "  'restricted',\n",
       "  'someone',\n",
       "  'sanders',\n",
       "  'violations',\n",
       "  'guns',\n",
       "  'political',\n",
       "  'cruise',\n",
       "  'innocent',\n",
       "  'direct'],\n",
       " 'YouShouldKnow': ['dog',\n",
       "  'flight',\n",
       "  'refund',\n",
       "  'bones',\n",
       "  'ocd',\n",
       "  'cat',\n",
       "  'credit',\n",
       "  'thoughts',\n",
       "  'mail',\n",
       "  'email',\n",
       "  'card',\n",
       "  'bleach',\n",
       "  'dogs',\n",
       "  'vet',\n",
       "  'airline',\n",
       "  'cancelled',\n",
       "  'cancel',\n",
       "  'pet',\n",
       "  'dispute',\n",
       "  'library',\n",
       "  'gas',\n",
       "  'file',\n",
       "  'goats',\n",
       "  'airlines',\n",
       "  'intrusive',\n",
       "  'goat',\n",
       "  'flights',\n",
       "  'head',\n",
       "  'courses',\n",
       "  'ebay',\n",
       "  'chlorine',\n",
       "  'voucher',\n",
       "  'app',\n",
       "  'ticket',\n",
       "  'tip',\n",
       "  'car',\n",
       "  'pets',\n",
       "  'cats',\n",
       "  'animals',\n",
       "  'snails',\n",
       "  'ducks',\n",
       "  'ammonia',\n",
       "  'service',\n",
       "  'bank',\n",
       "  'snail',\n",
       "  'thought',\n",
       "  'bone',\n",
       "  'feeding',\n",
       "  'honey',\n",
       "  'know',\n",
       "  'canceled',\n",
       "  'check',\n",
       "  'feed',\n",
       "  'certificate',\n",
       "  'rats',\n",
       "  'triangle',\n",
       "  'password',\n",
       "  'booked',\n",
       "  'charge',\n",
       "  'libraries',\n",
       "  'arrow',\n",
       "  'refunds',\n",
       "  'gym',\n",
       "  'credits',\n",
       "  'website',\n",
       "  'raw',\n",
       "  'kindle',\n",
       "  'filed',\n",
       "  'phone',\n",
       "  'delivery',\n",
       "  'info',\n",
       "  'mailbox',\n",
       "  'account',\n",
       "  'emails',\n",
       "  'wall',\n",
       "  'always',\n",
       "  'books',\n",
       "  'fitness',\n",
       "  'pain',\n",
       "  'harvard',\n",
       "  'nitrogen',\n",
       "  'cooked',\n",
       "  'customer',\n",
       "  'spray',\n",
       "  'amazon',\n",
       "  'sent',\n",
       "  'junk',\n",
       "  'payment',\n",
       "  'paypal',\n",
       "  'get',\n",
       "  'tumor',\n",
       "  'return',\n",
       "  'spirit',\n",
       "  'free',\n",
       "  'deer',\n",
       "  'scam',\n",
       "  'refunded',\n",
       "  'wild',\n",
       "  'cc',\n",
       "  'irs'],\n",
       " 'AskMen': ['relationship',\n",
       "  'men',\n",
       "  'makeup',\n",
       "  'women',\n",
       "  'girl',\n",
       "  'girls',\n",
       "  'friends',\n",
       "  'dating',\n",
       "  'date',\n",
       "  'woman',\n",
       "  'dad',\n",
       "  'heavy',\n",
       "  'girlfriend',\n",
       "  'relationships',\n",
       "  'attractive',\n",
       "  'partner',\n",
       "  'life',\n",
       "  'look',\n",
       "  'hair',\n",
       "  'learn',\n",
       "  'wear',\n",
       "  'talk',\n",
       "  'guy',\n",
       "  'man',\n",
       "  'feel',\n",
       "  'im',\n",
       "  'never',\n",
       "  'always',\n",
       "  'tell',\n",
       "  'likes',\n",
       "  'friend',\n",
       "  'wife',\n",
       "  'ask',\n",
       "  'started',\n",
       "  'fun',\n",
       "  'married',\n",
       "  'want',\n",
       "  'sports',\n",
       "  'wearing',\n",
       "  'dates',\n",
       "  'really',\n",
       "  'ex',\n",
       "  'good',\n",
       "  'skin',\n",
       "  'gf',\n",
       "  'beard',\n",
       "  'face',\n",
       "  'dated',\n",
       "  'time',\n",
       "  'pics',\n",
       "  'pictures',\n",
       "  'jets',\n",
       "  'father',\n",
       "  'together',\n",
       "  'compliment',\n",
       "  'attention',\n",
       "  'dick',\n",
       "  'hug',\n",
       "  'attracted',\n",
       "  'nice',\n",
       "  'herpes',\n",
       "  'feelings',\n",
       "  'try',\n",
       "  'sometimes',\n",
       "  'shower',\n",
       "  'met',\n",
       "  'shit',\n",
       "  'lot',\n",
       "  'fuck',\n",
       "  'sex',\n",
       "  'football',\n",
       "  'happy',\n",
       "  'know',\n",
       "  'confidence',\n",
       "  '20s',\n",
       "  'bro',\n",
       "  'insecure',\n",
       "  'liking',\n",
       "  'hobbies',\n",
       "  'dudes',\n",
       "  'mom',\n",
       "  'boyfriend',\n",
       "  'lol',\n",
       "  'comfortable',\n",
       "  'romantic',\n",
       "  'female',\n",
       "  'effort',\n",
       "  'guitar',\n",
       "  'beautiful',\n",
       "  'shave',\n",
       "  'make',\n",
       "  'told',\n",
       "  'felt',\n",
       "  'play',\n",
       "  'nfl',\n",
       "  'liked',\n",
       "  'things',\n",
       "  'hobby',\n",
       "  'sister',\n",
       "  'kiss'],\n",
       " 'Futurology': ['money',\n",
       "  'work',\n",
       "  'ubi',\n",
       "  'jobs',\n",
       "  'income',\n",
       "  'pay',\n",
       "  'tax',\n",
       "  'economy',\n",
       "  'people',\n",
       "  'month',\n",
       "  'rich',\n",
       "  'taxes',\n",
       "  'companies',\n",
       "  'nuclear',\n",
       "  'coal',\n",
       "  'energy',\n",
       "  'job',\n",
       "  'company',\n",
       "  'cost',\n",
       "  'rent',\n",
       "  'sick',\n",
       "  'solar',\n",
       "  'workers',\n",
       "  'working',\n",
       "  'prices',\n",
       "  'office',\n",
       "  'home',\n",
       "  'climate',\n",
       "  'government',\n",
       "  'yang',\n",
       "  'demand',\n",
       "  'per',\n",
       "  'power',\n",
       "  'inflation',\n",
       "  'employees',\n",
       "  'us',\n",
       "  'automation',\n",
       "  'wind',\n",
       "  'ai',\n",
       "  '000',\n",
       "  'need',\n",
       "  'universal',\n",
       "  'paid',\n",
       "  'wage',\n",
       "  'welfare',\n",
       "  'billion',\n",
       "  'robots',\n",
       "  'everyone',\n",
       "  'basic',\n",
       "  'price',\n",
       "  'business',\n",
       "  'afford',\n",
       "  'shit',\n",
       "  'dollars',\n",
       "  'electricity',\n",
       "  'costs',\n",
       "  'businesses',\n",
       "  'unemployment',\n",
       "  'cities',\n",
       "  'wealth',\n",
       "  'gas',\n",
       "  'increase',\n",
       "  'trillion',\n",
       "  'billionaires',\n",
       "  'goods',\n",
       "  'lawn',\n",
       "  'going',\n",
       "  'year',\n",
       "  'auto',\n",
       "  'renewables',\n",
       "  'emissions',\n",
       "  'healthcare',\n",
       "  'net',\n",
       "  'technology',\n",
       "  'corporations',\n",
       "  'hundred',\n",
       "  'storage',\n",
       "  'plants',\n",
       "  'grass',\n",
       "  'already',\n",
       "  'dress',\n",
       "  'world',\n",
       "  'labor',\n",
       "  'remote',\n",
       "  'live',\n",
       "  '2000',\n",
       "  'future',\n",
       "  'paying',\n",
       "  'gates',\n",
       "  'disrespectful',\n",
       "  'bill',\n",
       "  'fuel',\n",
       "  'moon',\n",
       "  'tech',\n",
       "  'automated',\n",
       "  'housing',\n",
       "  'cheaper',\n",
       "  'misinformation',\n",
       "  'country',\n",
       "  'million'],\n",
       " 'Art': ['art',\n",
       "  'plague',\n",
       "  'painting',\n",
       "  'photo',\n",
       "  'beautiful',\n",
       "  'picture',\n",
       "  'cd',\n",
       "  'artist',\n",
       "  'reminds',\n",
       "  'drawing',\n",
       "  'dragon',\n",
       "  'amazing',\n",
       "  'tv',\n",
       "  'paint',\n",
       "  'colors',\n",
       "  'nes',\n",
       "  'nostalgia',\n",
       "  'cool',\n",
       "  'midnight',\n",
       "  'wow',\n",
       "  'album',\n",
       "  'awesome',\n",
       "  'orange',\n",
       "  'double',\n",
       "  'piece',\n",
       "  'style',\n",
       "  '90s',\n",
       "  'carpet',\n",
       "  'tribe',\n",
       "  'nice',\n",
       "  'great',\n",
       "  'paintings',\n",
       "  'doctor',\n",
       "  'cheez',\n",
       "  'snes',\n",
       "  'draw',\n",
       "  'portrait',\n",
       "  'dope',\n",
       "  'pic',\n",
       "  'proportions',\n",
       "  'nintendo',\n",
       "  'lemons',\n",
       "  'artwork',\n",
       "  'doctors',\n",
       "  'photograph',\n",
       "  'game',\n",
       "  'vibes',\n",
       "  'photography',\n",
       "  'haha',\n",
       "  'critique',\n",
       "  'cassette',\n",
       "  'really',\n",
       "  '93',\n",
       "  'cds',\n",
       "  'dun',\n",
       "  'roman',\n",
       "  'color',\n",
       "  'look',\n",
       "  'quest',\n",
       "  'lighting',\n",
       "  'genesis',\n",
       "  'played',\n",
       "  'composition',\n",
       "  'mask',\n",
       "  'tape',\n",
       "  'damn',\n",
       "  'painted',\n",
       "  '1993',\n",
       "  'lol',\n",
       "  'vibe',\n",
       "  'belongs',\n",
       "  'shadows',\n",
       "  'playing',\n",
       "  'thought',\n",
       "  'image',\n",
       "  'drawings',\n",
       "  'inspired',\n",
       "  'incredible',\n",
       "  'bold',\n",
       "  'gorgeous',\n",
       "  'eyes',\n",
       "  'brush',\n",
       "  'upvote',\n",
       "  'digital',\n",
       "  'wonderful',\n",
       "  'anatomy',\n",
       "  'modern',\n",
       "  'canvas',\n",
       "  'photoshop',\n",
       "  'work',\n",
       "  'floor',\n",
       "  'viewer',\n",
       "  'talented',\n",
       "  'pencil',\n",
       "  'smells',\n",
       "  'textures',\n",
       "  'dude',\n",
       "  'albums',\n",
       "  'era',\n",
       "  'nsfw'],\n",
       " 'IAmA': ['virus',\n",
       "  'hi',\n",
       "  'symptoms',\n",
       "  'covid',\n",
       "  'question',\n",
       "  '19',\n",
       "  'favorite',\n",
       "  'hospital',\n",
       "  '5g',\n",
       "  'infected',\n",
       "  'game',\n",
       "  'book',\n",
       "  'hey',\n",
       "  'questions',\n",
       "  'film',\n",
       "  'bipolar',\n",
       "  'ray',\n",
       "  'movie',\n",
       "  'flu',\n",
       "  'patients',\n",
       "  'disorder',\n",
       "  'masks',\n",
       "  'norm',\n",
       "  'contact',\n",
       "  'hello',\n",
       "  'ama',\n",
       "  'quarantine',\n",
       "  'stay',\n",
       "  'testing',\n",
       "  'answer',\n",
       "  'mask',\n",
       "  'hospitals',\n",
       "  'tested',\n",
       "  'infection',\n",
       "  'hope',\n",
       "  'italy',\n",
       "  'days',\n",
       "  'weeks',\n",
       "  'advice',\n",
       "  'help',\n",
       "  'union',\n",
       "  'coronavirus',\n",
       "  'fan',\n",
       "  'costco',\n",
       "  'episode',\n",
       "  'recommend',\n",
       "  'wash',\n",
       "  'show',\n",
       "  'fever',\n",
       "  'hands',\n",
       "  'spread',\n",
       "  'census',\n",
       "  'cough',\n",
       "  'cases',\n",
       "  'surfaces',\n",
       "  'curve',\n",
       "  'safe',\n",
       "  'ppe',\n",
       "  'movies',\n",
       "  'risk',\n",
       "  'home',\n",
       "  'nurses',\n",
       "  'podcast',\n",
       "  'doctor',\n",
       "  'staff',\n",
       "  'russian',\n",
       "  'wondering',\n",
       "  'gary',\n",
       "  'isolation',\n",
       "  'patient',\n",
       "  'bot',\n",
       "  'films',\n",
       "  'town',\n",
       "  'cdc',\n",
       "  'ever',\n",
       "  'tests',\n",
       "  'mood',\n",
       "  'answering',\n",
       "  'episodes',\n",
       "  'toothbrush',\n",
       "  'immunity',\n",
       "  'trailer',\n",
       "  'health',\n",
       "  'lego',\n",
       "  'immune',\n",
       "  'favourite',\n",
       "  'know',\n",
       "  'medication',\n",
       "  'distancing',\n",
       "  'icu',\n",
       "  'pandemic',\n",
       "  'respiratory',\n",
       "  'n95',\n",
       "  'best',\n",
       "  'studio',\n",
       "  'concerns',\n",
       "  'touch',\n",
       "  'dr',\n",
       "  'app',\n",
       "  'gloves'],\n",
       " 'science': ['study',\n",
       "  'science',\n",
       "  'cells',\n",
       "  'species',\n",
       "  'data',\n",
       "  'article',\n",
       "  'facebook',\n",
       "  'vaccine',\n",
       "  'quantum',\n",
       "  'research',\n",
       "  'ego',\n",
       "  'bats',\n",
       "  'studies',\n",
       "  'viruses',\n",
       "  'meat',\n",
       "  'lab',\n",
       "  'wet',\n",
       "  'humans',\n",
       "  'math',\n",
       "  'test',\n",
       "  'animals',\n",
       "  'researchers',\n",
       "  'antibodies',\n",
       "  'results',\n",
       "  'vaccines',\n",
       "  'co2',\n",
       "  'energy',\n",
       "  'immune',\n",
       "  'cats',\n",
       "  'carbon',\n",
       "  'effects',\n",
       "  'anecdotes',\n",
       "  'nuclear',\n",
       "  'respectful',\n",
       "  'plant',\n",
       "  'theory',\n",
       "  'markets',\n",
       "  'memes',\n",
       "  'sars',\n",
       "  'tests',\n",
       "  'virus',\n",
       "  'trials',\n",
       "  'rna',\n",
       "  'viral',\n",
       "  'bacteria',\n",
       "  'scientists',\n",
       "  'dna',\n",
       "  'animal',\n",
       "  'nightmares',\n",
       "  'brain',\n",
       "  'effect',\n",
       "  'plants',\n",
       "  'protein',\n",
       "  'smell',\n",
       "  'sample',\n",
       "  'soil',\n",
       "  'farming',\n",
       "  'cov',\n",
       "  'gravity',\n",
       "  'encourage',\n",
       "  'fasting',\n",
       "  'bat',\n",
       "  'teachers',\n",
       "  'drug',\n",
       "  'infection',\n",
       "  'disease',\n",
       "  'schools',\n",
       "  'sleep',\n",
       "  'patients',\n",
       "  'coal',\n",
       "  'proteins',\n",
       "  'enzyme',\n",
       "  'human',\n",
       "  'climate',\n",
       "  'methane',\n",
       "  'scientific',\n",
       "  'testing',\n",
       "  'emissions',\n",
       "  'atmosphere',\n",
       "  'mice',\n",
       "  'nazis',\n",
       "  'temperature',\n",
       "  'cell',\n",
       "  'covid',\n",
       "  'participants',\n",
       "  'paper',\n",
       "  'crops',\n",
       "  'austria',\n",
       "  'eating',\n",
       "  'masks',\n",
       "  'homo',\n",
       "  '500',\n",
       "  'abstract',\n",
       "  'blood',\n",
       "  'outcomes',\n",
       "  'livestock',\n",
       "  'standardized',\n",
       "  'diet',\n",
       "  'particles',\n",
       "  'crispr'],\n",
       " 'Cooking': ['chicken',\n",
       "  'cook',\n",
       "  'cooking',\n",
       "  'add',\n",
       "  'sauce',\n",
       "  'rice',\n",
       "  'meat',\n",
       "  'salt',\n",
       "  'recipe',\n",
       "  'cheese',\n",
       "  'eggs',\n",
       "  'oil',\n",
       "  'pasta',\n",
       "  'butter',\n",
       "  'pan',\n",
       "  'cooked',\n",
       "  'heat',\n",
       "  'use',\n",
       "  'steak',\n",
       "  'taste',\n",
       "  'garlic',\n",
       "  'recipes',\n",
       "  'dish',\n",
       "  'food',\n",
       "  'beef',\n",
       "  'oven',\n",
       "  'flavor',\n",
       "  'ingredients',\n",
       "  'egg',\n",
       "  'bread',\n",
       "  'water',\n",
       "  'pepper',\n",
       "  'fresh',\n",
       "  'cream',\n",
       "  'pork',\n",
       "  'pot',\n",
       "  'soup',\n",
       "  'dry',\n",
       "  'pizza',\n",
       "  'kitchen',\n",
       "  'dishes',\n",
       "  'hot',\n",
       "  'delicious',\n",
       "  'baking',\n",
       "  'flour',\n",
       "  'minutes',\n",
       "  'wine',\n",
       "  'eat',\n",
       "  'potatoes',\n",
       "  'sugar',\n",
       "  'mix',\n",
       "  'beans',\n",
       "  'iron',\n",
       "  'brown',\n",
       "  'onions',\n",
       "  'tomato',\n",
       "  'make',\n",
       "  'frozen',\n",
       "  'onion',\n",
       "  'fried',\n",
       "  'knife',\n",
       "  'bacon',\n",
       "  'texture',\n",
       "  'cast',\n",
       "  'powder',\n",
       "  'fry',\n",
       "  'medium',\n",
       "  'tomatoes',\n",
       "  'fridge',\n",
       "  'cup',\n",
       "  'salad',\n",
       "  'cut',\n",
       "  'spices',\n",
       "  'olive',\n",
       "  'meal',\n",
       "  'vinegar',\n",
       "  'chili',\n",
       "  'freeze',\n",
       "  'stir',\n",
       "  'chef',\n",
       "  'bowl',\n",
       "  'good',\n",
       "  'steel',\n",
       "  'try',\n",
       "  'roast',\n",
       "  'veggies',\n",
       "  'fat',\n",
       "  'canned',\n",
       "  'thin',\n",
       "  'red',\n",
       "  'fish',\n",
       "  'bake',\n",
       "  'italian',\n",
       "  'yeast',\n",
       "  'angel',\n",
       "  'milk',\n",
       "  'lemon',\n",
       "  'juice',\n",
       "  'seasoning',\n",
       "  'temp']}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_per_by_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    sub_reddit = row['subreddit']\n",
    "    tokens = tknzr.tokenize(row['body'])\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in topic_per_by_subreddit[sub_reddit]:\n",
    "            new_tokens.append(\"TOPIC\")\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    row['body'] = \" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I thought this was already the case ? Had it b...</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>dublea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The original post had the moratorum lasting un...</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>Ansuz07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Technically , the H1N1 pandemic of 2009 was on...</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>scott60561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The WHO declared an end of the H1N1 pandemic (...</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>SaxonySam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Thank you for providing TOPIC with sources So ...</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>epmuscle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body     subreddit      author\n",
       "0  I thought this was already the case ? Had it b...  changemyview      dublea\n",
       "1  The original post had the moratorum lasting un...  changemyview     Ansuz07\n",
       "2  Technically , the H1N1 pandemic of 2009 was on...  changemyview  scott60561\n",
       "3  The WHO declared an end of the H1N1 pandemic (...  changemyview   SaxonySam\n",
       "4  Thank you for providing TOPIC with sources So ...  changemyview    epmuscle"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_duplicate_topic:\n",
    "    filename = 'after_non_duplicate_top' + str(topk) + '_removal_large.csv'\n",
    "else: \n",
    "    filename = 'after_top' + str(topk) + '_removal_large.csv'\n",
    "df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
